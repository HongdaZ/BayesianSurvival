\documentclass{beamer}

\mode<presentation> {
	
	% The Beamer class comes with a number of default slide themes
	% which change the colors and layouts of slides. Below this is a list
	% of all the themes, uncomment each in turn to see what they look like.
	
	%\usetheme{default}
	%\usetheme{AnnArbor}
	%\usetheme{Antibes}
	%\usetheme{Bergen}
	%\usetheme{Berkeley}
	%\usetheme{Berlin}
	%\usetheme{Boadilla}
	%\usetheme{CambridgeUS}
	%\usetheme{Copenhagen}
	%\usetheme{Darmstadt}
	%\usetheme{Dresden}
	%\usetheme{Frankfurt}
	%\usetheme{Goettingen}
	%\usetheme{Hannover}
	%\usetheme{Ilmenau}
	%\usetheme{JuanLesPins}
	%\usetheme{Luebeck}
	\usetheme{Madrid}
	%\usetheme{Malmoe}
	%\usetheme{Marburg}
	%\usetheme{Montpellier}
	%\usetheme{PaloAlto}
	%\usetheme{Pittsburgh}
	%\usetheme{Rochester}
	%\usetheme{Singapore}
	%\usetheme{Szeged}
	%\usetheme{Warsaw}
	
	% As well as themes, the Beamer class has a number of color themes
	% for any slide theme. Uncomment each of these in turn to see how it
	% changes the colors of your current slide theme.
	
	%\usecolortheme{albatross}
	%\usecolortheme{beaver}
	%\usecolortheme{beetle}
	%\usecolortheme{crane}
	%\usecolortheme{dolphin}
	%\usecolortheme{dove}
	%\usecolortheme{fly}
	%\usecolortheme{lily}
	%\usecolortheme{orchid}
	%\usecolortheme{rose}
	%\usecolortheme{seagull}
	%\usecolortheme{seahorse}
	%\usecolortheme{whale}
	%\usecolortheme{wolverine}
	
	%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
	%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line
	
	%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables 

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\setbeamertemplate{caption}[numbered]
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage[toc,page]{appendix}
\usepackage{epstopdf}
\usepackage{latexsym}
\usepackage{amstext}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{float}
%\usepackage{hyperref} give reference hyperlink 
%\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{color}
\usepackage{mathtools}
% packages from template
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage[width=.9\textwidth]{caption}
\usepackage{mathrsfs}
\usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\usepackage{textgreek}
\usepackage{bbold}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[algo2e,ruled,vlined]{algorithm2e} 
\usepackage{fancyvrb}


\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\M}{\boldsymbol{M}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\rep}{\mathrm{rep}}
\newcommand{\PR}{\text{Pr}}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
%\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\bs}[1]{\pmb{#1}}
\newcommand{\mb}[1]{\boldsymbol{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\allowdisplaybreaks
%=============================================================================
% prelude
%=============================================================================
\def\mathLarge#1{\mbox{\LARGE $#1$}}
\usepackage{soul}


\title[]{Bayesian Survival Analysis}
\author[Hongda Zhang]{Hongda Zhang}
\institute{Nanjing University}
\date{\today}



\begin{document}
	\begin{frame}
		\titlepage
	\end{frame}
	
	\begin{frame}
		\frametitle{Contents}
		\begin{enumerate}
			\item Introduction
			\item Parametric Models
			\item Semi-parametric Models
		\end{enumerate}
		\nocite{*}
	\end{frame}
	
	\begin{frame}
		\frametitle{Introduction}
		The likelihood function of the observed data $D$, given a vector of unknown parameters $\boldsymbol{\theta}$, is denoted by $L(\boldsymbol{\theta}|D)$. In contrast to frequentist statistics, $\boldsymbol{\theta}$ is also treated as random with a prior distribution $\pi(\boldsymbol{\theta})$. Then the inference of $\boldsymbol{\theta}$ is based on the \emph{posterior} distribution 
		\[
		\pi(\boldsymbol{\theta} | D) = \frac{L(\boldsymbol{\theta} | D)\pi(\boldsymbol{\theta})}{\int_{\boldsymbol{\Theta}}L(\boldsymbol{\theta} | D)\pi(\boldsymbol{\theta}) d\boldsymbol{\theta}}
		\]
		It's obvious that 
		\[
		\pi(\boldsymbol{\theta} | D) \propto L(\boldsymbol{\theta} | D)\pi(\boldsymbol{\theta}).
		\]
		In most cases, $m(D) = \int_{\boldsymbol{\Theta}}L(\boldsymbol{\theta} | D)\pi(\boldsymbol{\theta}) d\boldsymbol{\theta}$ doesn't have a closed form expression. Then the problem becomes how to sample from the posterior distribution $\pi(\boldsymbol{\theta} | D)$. Many Markov chain Monte Carlo (MCMC) methods were proposed to do the work.
	\end{frame}
	
	\begin{frame}
		\frametitle{Introduction}
		For making a prediction of a vector of new observations $\boldsymbol{z}$, we need to derive the posterior predictive distribution of it given the observed data D
		\[
		\pi(\boldsymbol{z} | D) = \int_{\boldsymbol{\Theta}}f(\boldsymbol{z} | \boldsymbol{\theta}) \pi(\boldsymbol{\theta} | D) d\boldsymbol{\theta},
		\] 
		where $f(\boldsymbol{z} | \boldsymbol{\theta})$ is the likelihood function of the unknown parameters $\boldsymbol{\theta}$. Essentially, $\pi(\boldsymbol{z} | D)$ is the posterior expectation of $f(\boldsymbol{z} | \boldsymbol{\theta})$. Thus we can draw samples from $\pi(\boldsymbol{z} | D)$ through samples from $ \pi(\boldsymbol{\theta} | D)$.
	\end{frame}
	
	\begin{frame}
		\frametitle{Introduction}
		Advantages of Bayesian paradigm
		\begin{itemize}
			\item It is well known that survival models are very hard to fit, especially with the presence of complex censoring schemes. With the MCMC methods, fitting the survival data is straight forward.
			\item Frequentist methods require large sample size while MCMC sampling methods can help make exact inference for any sample size.
			\item  In frequentist paradigm, variance is estimated asymptotically. On the contrary, it's trivial to sample from posterior distributions using MCMC methods and the variance is easily obtained.
			\item Using Bayesian paradigm, it's natural to incorporate prior information by specifying prior distributions.
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Introduction}
		Advantages of Bayesian paradigm (cont.)
		\begin{itemize}
			\item Frequentist inference can be regarded as a special case of Bayesian inference for many models. 
			\item In the Bayesian paradigm, model selection is easily done via Bayes factors or other criteria, e.g. Bayesian information criterion (BIC). The Bayes factors and model selection criteria can be exactly computed via MCMC methods. On the other hand, there is no unified methodology for comparing non-nested models in frequentist paradigm. Furthermore, comparison of nested models in frequentist paradigm usually require asymptotic arguments which rely on a large sample size.
			\item  In principle, missing covariate or response data is not a big problem that they are treated as unknown parameters in Bayesian paradigm. The missing values only require an extra layer in the sampling methods. In contrast, dealing with missing values in frequentist paradigm is quite complicated and require more computationally intensive methods.
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		The most fundamental parametric model in survival analysis is exponential model. Suppose there are independent identically distributed  (i.i.d.) survival times $\boldsymbol{y} = (y_1, y_2, \dots, y_n)'$, each having an exponential distribution with parameter $\lambda$, denoted by $\mathcal{E}(\lambda)$. Let $\boldsymbol{\nu} = (\nu_1, \nu_2, \dots, \nu_n)'$ denote the censoring indicators, where $\nu_i = 0$ is $y_i$ is right censored and $\nu_i = 1$ if $y_i$ is a failure time. Let $f(y_i | \lambda) = \lambda\exp(-\lambda y_i)$ denote the density for $y_i$, $S(y_i | \lambda) = \exp(-\lambda y_i)$ denotes the survival function and $D = (n, \boldsymbol{y}, \boldsymbol{\nu})$ denotes the observed data. Then the likelihood function of $\lambda$ is given by
		\[
		L(\lambda | D) \prod_{i = 1}^{n} f(y_i | \lambda) ^ { \nu_i} S(y_i | \lambda) ^ {(1 - \nu_i)} = \lambda ^ d \exp(\lambda\sum_{i = 1}^{n}y_i),
		\]
		where $d = \sum_{i = 1}^{n}\nu_i$.
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		The gamma prior is the conjugate prior for $\lambda$. Let $\mathcal{G}(\alpha_0, \lambda_0)$ denote the gamma distribution with parameters $(\alpha_0, \lambda_0)$. Then the density function is given by
		\[
		\pi(\lambda | \alpha_0, \lambda_0) \propto \lambda ^ {\alpha_0 - 1} \exp(-\lambda_0\lambda).
		\]
		Then the posterior distribution is given by
		\begin{align*}
			\pi(\lambda | D) & \propto L(\lambda | D) \pi(\lambda | \alpha_0, \lambda_0) \\
			& \propto ( \lambda ^ {\sum_{i = 1}^{n}}\exp\{-\lambda\sum_{i = 1}^{n}y_i\})(\lambda ^ {\alpha_0 - 1}\exp( -\lambda_0\lambda)) \\
			& = \lambda ^ {\alpha_0 + d - 1}\exp\{\lambda( \lambda_0 + \sum_{i = 1}^{n}y_i)\}.
		\end{align*}
	    Obviously, it is $\mathcal{G}(\alpha_0 + d, \lambda_0 + \sum_{i = 1}^{n}y_i)$.
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		The posterior mean and variance are easily obtained by
		\[
		E(\lambda | D) = \frac{\alpha_0 + d}{\lambda_0 + \sum_{i = 1}^{n}y_i}
		\]
		and 
		\[
		\text{Var}(\lambda | D) = \frac{\alpha_0 + d}{(\lambda_0 + \sum_{i = 1}^{n}y_i) ^ 2}.
		\]
		The posterior predictive distribution of a new failure time $y_f$ is given by
		\begin{align*}
			\pi(y_f | D) & = \int_{0}^{\infty}\pi(y_f | \lambda) \pi(\lambda | D) d\lambda \\
			& \propto\int_{0}^{\infty} \lambda ^ {\alpha_0 + d + 1 - 1}\exp\{-\lambda(y_f + \lambda_0 + \sum_{i = 1}^{n}y_i)\}d\lambda \\
			& = \Gamma(\alpha_0 + d + 1)(\lambda_0 + \sum_{i = 1}^{n}y_i + y_f) ^ { -(d + \alpha_0 + 1)} \\
			& \propto (\lambda_0 + \sum_{i = 1}^{n}y_i + y_f) ^ {-d + \alpha_0 + 1},
		\end{align*}
		which is an inverse beta distribution.
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		Covariates can be introduced through $\lambda$ to build a regression model. Let $\boldsymbol{x}_i$ denote a $p\times1$ vector of covariates, $\boldsymbol{\beta}$ denote a $p\times1$ vector of regression coefficients, and $\phi(\cdot)$ denote a known function. Then let $\lambda_i = \phi( \boldsymbol{x}_i'\boldsymbol{\beta})$. Usually, $\phi(\cdot)$ takes the form of $\exp(\cdot)$. Then the likelihood function of regression coefficients is given by 
		\begin{align*}
			L(\boldsymbol{\beta} | D) & = \prod_{i = 1}^{n}f(y_i | \lambda_i) ^ {v_i}S(y_i | \lambda_i) ^ {1 - v_i} \\
			& = \exp\{\sum_{i = 1}^{n}\nu_i\boldsymbol{x}_i'\boldsymbol{\beta}\}\exp\{-\sum_{i = 1}^{n}y_i\exp(\boldsymbol{x}_i'\boldsymbol{\beta})\}.
		\end{align*}
		Suppose we specify a $p-$dimensional normal prior for $\boldsymbol{\beta}$, denoted by $N_p(\boldsymbol{\mu}_0, \Sigma_0)$, with 
		$\boldsymbol{\mu}_0$ as prior mean and $\Sigma_0$ as prior covariance matrix. Thus the posterior distribution of $\boldsymbol{\beta}$ is given by 
		\[
		\pi(\boldsymbol{\beta} | D) \propto L(\boldsymbol{\beta} | D)\pi(\boldsymbol{\beta} | \boldsymbol{\mu_0, \Sigma_0}),
		\]
		where $\pi(\boldsymbol{\beta} | \boldsymbol{\mu_0, \Sigma_0})$ is the multivariate normal distribution.
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		Covariates can be introduced through $\lambda$ to build a regression model. Let $\boldsymbol{x}_i$ denote a $p\times1$ vector of covariates, $\boldsymbol{\beta}$ denote a $p\times1$ vector of regression coefficients, and $\phi(\cdot)$ denote a known function. Then let $\lambda_i = \phi( \boldsymbol{x}_i'\boldsymbol{\beta})$. Usually, $\phi(\cdot)$ takes the form of $\exp(\cdot)$. Then the likelihood function of regression coefficients is given by 
		\begin{align*}
			L(\boldsymbol{\beta} | D) & = \prod_{i = 1}^{n}f(y_i | \lambda_i) ^ {v_i}S(y_i | \lambda_i) ^ {1 - v_i} \\
			& = \exp\{\sum_{i = 1}^{n}\nu_i\boldsymbol{x}_i'\boldsymbol{\beta}\}\exp\{-\sum_{i = 1}^{n}y_i\exp(\boldsymbol{x}_i'\boldsymbol{\beta})\}.
		\end{align*}
		Suppose we specify a $p-$dimensional normal prior for $\boldsymbol{\beta}$, denoted by $N_p(\boldsymbol{\mu}_0, \Sigma_0)$, with 
		$\boldsymbol{\mu}_0$ as prior mean and $\Sigma_0$ as prior covariance matrix. Thus the posterior distribution of $\boldsymbol{\beta}$ is given by 
		\[
		\pi(\boldsymbol{\beta} | D) \propto L(\boldsymbol{\beta} | D)\pi(\boldsymbol{\beta} | \boldsymbol{\mu_0, \Sigma_0}),
		\]
		where $\pi(\boldsymbol{\beta} | \boldsymbol{\mu_0, \Sigma_0})$ is the multivariate normal distribution.
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		The Weibull model is widely used and more flexible than the exponential model. Suppose there are i.i.d. random variables $\boldsymbol{y} = (y_1, y_2, \dots, y_n)'$, each having a Weibull distribution, denoted by $\mathcal{W}(\alpha, \gamma)$. We reparameterize the model by letting $\lambda = \log(\gamma)$. Then the density function of $y_i$ is given by 
		\[
		f(y_i | \alpha, \lambda) = \alpha y ^ {\alpha - 1}\exp(\lambda - \exp(\lambda)y ^ {\alpha}).
		\] 
		The likelihood function is given by
		\begin{align*}
			L(\alpha, \lambda | D) & = \prod_{i = 1}^{n}f(y_i | \alpha, \lambda) ^ {\nu_i}S(y_i | \alpha, \lambda) ^ {1 - \nu_i} \\
			& = \alpha ^ d \exp\{d\lambda + \sum_{i = 1}^{n}(\nu_i(\alpha - 1)\log(y_i) - \exp(\lambda)y_i ^ {\alpha})\}.
		\end{align*}
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		If we specify gamma prior, $\mathcal{G}(\alpha_0, \kappa_0)$, for $\alpha$ and normal prior, $N(\mu_0, \sigma_0 ^ 2)$, for $\lambda$. Then the joint posterior distribution of $(\alpha, \lambda)$ is given by
		\begin{equation*}
			\begin{split}
				\pi(\alpha, \lambda | D) & \propto L(\alpha, \lambda | D) \pi(\alpha | \alpha_0, \kappa_0)\pi(\lambda | \mu_0, \sigma_0 ^ 2) \\
				& = \alpha ^ {\alpha_0 + d - 1} \exp\{d\lambda + \sum_{i = 1}^{n}(\nu_i(\alpha - 1)\log(y_i) - \exp(\lambda)y_i ^ {\alpha}) \\ 
				& \quad\quad\quad\quad\quad - \kappa_0\alpha - \frac{1}{2\sigma_0 ^ 2}(\lambda - \mu_0) ^ 2\}.
			\end{split}
		\end{equation*}
	\end{frame}
	
	\begin{frame}
		\frametitle{Parametric Models}
		To build a regression model, covariates $\boldsymbol{x}_i$ can be introduced through $\lambda$ with $\lambda_i = \boldsymbol{x}_i'\boldsymbol{\beta}$. If we assume a $N_p(\mu_0, \Sigma_0)$ prior for $\boldsymbol{\beta}$ and a $\mathcal{G}(\alpha_0, \kappa_0)$ prior for $\alpha$, then we are let to the joint posterior distribution
		\begin{equation*}
			\begin{split}
				\pi(\boldsymbol{\beta, \alpha | D}) & \propto \alpha ^ {\alpha_0 + d - 1}\exp\{\sum_{i = 1}^{n}(\nu_i\boldsymbol{x}_i'\boldsymbol{\beta} + \nu_i(\alpha - 1)\log(y_i) - y_i ^ {\alpha}\exp( \boldsymbol{x}_i'\boldsymbol{\beta})) \\
				& \quad\quad - \kappa_0\alpha - \frac{1}{2}(\boldsymbol{\beta} - \boldsymbol{\mu_0})' \Sigma_0 ^ {-1}(\boldsymbol{\beta} - \boldsymbol{\mu_0})\}.			
			\end{split}
		\end{equation*}
	\end{frame}

	\begin{frame}[allowframebreaks]
		\begin{singlespace}
			\interlinepenalty=10000	% prevents bib items from splitting across pages
			\bibliography{BayesianSurvival}
			\bibliographystyle{apalike}
		\end{singlespace}
	\end{frame}
	
\end{document} 